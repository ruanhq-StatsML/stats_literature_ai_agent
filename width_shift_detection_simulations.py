"""
Width-Based Distribution Shift Detection: Simulation Studies
=============================================================

This module implements and validates the theoretical results from:
"Prediction Interval Width as a Distribution Shift Detector"

Validates:
1. Type I error control (should be ≤ α under H0)
2. Power under local alternatives
3. Asymptotic distribution of test statistic
4. Comparison across different prediction interval methods

Author: Generated by stats_literature_ai_agent
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import norm, kstest
from sklearn.linear_model import QuantileRegressor, LinearRegression
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import train_test_split
from dataclasses import dataclass
from typing import Tuple, List, Optional, Callable, Dict
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)


# =============================================================================
# DATA CLASSES AND TYPE DEFINITIONS
# =============================================================================

@dataclass
class ShiftDetectionResult:
    """Result of shift detection test."""
    test_statistic: float
    p_value: float
    shift_detected: bool
    mean_width_cal: float
    mean_width_test: float
    std_width_cal: float
    std_width_test: float


@dataclass
class SimulationResult:
    """Result of simulation study."""
    type1_error: float
    power_curve: np.ndarray
    shift_magnitudes: np.ndarray
    test_stats_null: np.ndarray
    test_stats_alt: Dict[float, np.ndarray]


# =============================================================================
# PREDICTION INTERVAL METHODS
# =============================================================================

class PredictionIntervalMethod:
    """Base class for prediction interval methods."""

    def fit(self, X: np.ndarray, y: np.ndarray, alpha: float = 0.1):
        """Fit the model."""
        raise NotImplementedError

    def predict_interval(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Return (lower, upper) bounds."""
        raise NotImplementedError

    def get_widths(self, X: np.ndarray) -> np.ndarray:
        """Get prediction interval widths."""
        lower, upper = self.predict_interval(X)
        return upper - lower


class QuantileRegressionInterval(PredictionIntervalMethod):
    """Quantile regression-based prediction intervals."""

    def __init__(self):
        self.model_lower = None
        self.model_upper = None
        self.alpha = 0.1

    def fit(self, X: np.ndarray, y: np.ndarray, alpha: float = 0.1):
        self.alpha = alpha
        # Use sklearn's QuantileRegressor
        self.model_lower = QuantileRegressor(quantile=alpha/2, alpha=0.01, solver='highs')
        self.model_upper = QuantileRegressor(quantile=1-alpha/2, alpha=0.01, solver='highs')
        self.model_lower.fit(X, y)
        self.model_upper.fit(X, y)
        return self

    def predict_interval(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        lower = self.model_lower.predict(X)
        upper = self.model_upper.predict(X)
        return lower, upper


class BootstrapInterval(PredictionIntervalMethod):
    """Bootstrap-based prediction intervals."""

    def __init__(self, n_bootstrap: int = 100):
        self.n_bootstrap = n_bootstrap
        self.models = []
        self.alpha = 0.1

    def fit(self, X: np.ndarray, y: np.ndarray, alpha: float = 0.1):
        self.alpha = alpha
        self.models = []
        n = len(y)

        for _ in range(self.n_bootstrap):
            idx = np.random.choice(n, size=n, replace=True)
            model = LinearRegression()
            model.fit(X[idx], y[idx])
            self.models.append(model)

        return self

    def predict_interval(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        predictions = np.array([m.predict(X) for m in self.models])
        lower = np.percentile(predictions, 100 * self.alpha/2, axis=0)
        upper = np.percentile(predictions, 100 * (1 - self.alpha/2), axis=0)
        return lower, upper


class HeteroscedasticInterval(PredictionIntervalMethod):
    """Heteroscedastic model-based prediction intervals."""

    def __init__(self):
        self.mean_model = None
        self.var_model = None
        self.alpha = 0.1

    def fit(self, X: np.ndarray, y: np.ndarray, alpha: float = 0.1):
        self.alpha = alpha

        # Fit mean model
        self.mean_model = LinearRegression()
        self.mean_model.fit(X, y)

        # Fit variance model on squared residuals
        residuals = y - self.mean_model.predict(X)
        log_sq_residuals = np.log(residuals**2 + 1e-8)

        self.var_model = LinearRegression()
        self.var_model.fit(X, log_sq_residuals)

        return self

    def predict_interval(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        mean_pred = self.mean_model.predict(X)
        log_var_pred = self.var_model.predict(X)
        std_pred = np.sqrt(np.exp(log_var_pred))

        z = norm.ppf(1 - self.alpha/2)
        lower = mean_pred - z * std_pred
        upper = mean_pred + z * std_pred
        return lower, upper


class ConformalInterval(PredictionIntervalMethod):
    """Split conformal prediction intervals."""

    def __init__(self):
        self.base_model = None
        self.quantile = None
        self.alpha = 0.1

    def fit(self, X: np.ndarray, y: np.ndarray, alpha: float = 0.1):
        self.alpha = alpha

        # Split into training and calibration
        X_train, X_cal, y_train, y_cal = train_test_split(
            X, y, test_size=0.5, random_state=42
        )

        # Fit base model
        self.base_model = LinearRegression()
        self.base_model.fit(X_train, y_train)

        # Compute calibration scores
        y_cal_pred = self.base_model.predict(X_cal)
        scores = np.abs(y_cal - y_cal_pred)

        # Compute quantile
        n_cal = len(scores)
        self.quantile = np.quantile(
            scores,
            np.ceil((1 - alpha) * (n_cal + 1)) / n_cal,
            method='higher'
        )

        return self

    def predict_interval(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        y_pred = self.base_model.predict(X)
        lower = y_pred - self.quantile
        upper = y_pred + self.quantile
        return lower, upper


# =============================================================================
# SHIFT DETECTION ALGORITHMS
# =============================================================================

def compute_width_ratio_statistic(
    widths_cal: np.ndarray,
    widths_test: np.ndarray
) -> float:
    """
    Compute the width-ratio test statistic.

    T_WR = mean(widths_test) / mean(widths_cal)
    """
    return np.mean(widths_test) / np.mean(widths_cal)


def permutation_test(
    widths_cal: np.ndarray,
    widths_test: np.ndarray,
    n_permutations: int = 1000,
    statistic_fn: Callable = compute_width_ratio_statistic
) -> Tuple[float, float]:
    """
    Permutation test for shift detection.

    Returns: (test_statistic, p_value)
    """
    # Observed statistic
    t_obs = statistic_fn(widths_cal, widths_test)

    # Pool all widths
    all_widths = np.concatenate([widths_cal, widths_test])
    n_cal = len(widths_cal)
    n_total = len(all_widths)

    # Permutation distribution
    count = 0
    for _ in range(n_permutations):
        perm = np.random.permutation(n_total)
        w_cal_perm = all_widths[perm[:n_cal]]
        w_test_perm = all_widths[perm[n_cal:]]
        t_perm = statistic_fn(w_cal_perm, w_test_perm)

        if t_perm >= t_obs:
            count += 1

    # p-value with +1 correction for exactness
    p_value = (count + 1) / (n_permutations + 1)

    return t_obs, p_value


def detect_shift(
    X_cal: np.ndarray,
    y_cal: np.ndarray,
    X_test: np.ndarray,
    interval_method: PredictionIntervalMethod,
    alpha: float = 0.1,
    beta: float = 0.05,
    n_permutations: int = 1000
) -> ShiftDetectionResult:
    """
    Full shift detection pipeline.

    Args:
        X_cal: Calibration covariates
        y_cal: Calibration responses (for fitting)
        X_test: Test covariates
        interval_method: Prediction interval method to use
        alpha: Prediction interval confidence level
        beta: Significance level for shift test
        n_permutations: Number of permutations

    Returns:
        ShiftDetectionResult with test statistic, p-value, and diagnostics
    """
    # Fit interval method
    interval_method.fit(X_cal, y_cal, alpha=alpha)

    # Compute widths
    widths_cal = interval_method.get_widths(X_cal)
    widths_test = interval_method.get_widths(X_test)

    # Permutation test
    t_obs, p_value = permutation_test(
        widths_cal, widths_test, n_permutations
    )

    return ShiftDetectionResult(
        test_statistic=t_obs,
        p_value=p_value,
        shift_detected=(p_value < beta),
        mean_width_cal=np.mean(widths_cal),
        mean_width_test=np.mean(widths_test),
        std_width_cal=np.std(widths_cal),
        std_width_test=np.std(widths_test)
    )


# =============================================================================
# DATA GENERATION
# =============================================================================

def generate_regression_data(
    n: int,
    d: int = 5,
    noise_std: float = 1.0,
    mean_shift: float = 0.0,
    cov_scale: float = 1.0,
    heteroscedastic: bool = False
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate regression data with optional covariate shift.

    Y = X @ beta + epsilon

    Args:
        n: Number of samples
        d: Dimension of covariates
        noise_std: Standard deviation of noise
        mean_shift: Mean shift in covariates (||mu||)
        cov_scale: Scale of covariate covariance
        heteroscedastic: If True, noise depends on X
    """
    # Covariates with potential shift
    mu = np.zeros(d)
    if mean_shift > 0:
        mu[0] = mean_shift  # Shift in first coordinate

    X = np.random.multivariate_normal(mu, cov_scale * np.eye(d), size=n)

    # True regression function
    beta = np.ones(d) / np.sqrt(d)
    y_mean = X @ beta

    # Noise
    if heteroscedastic:
        noise_scale = noise_std * (1 + 0.5 * np.abs(X[:, 0]))
    else:
        noise_scale = noise_std * np.ones(n)

    y = y_mean + noise_scale * np.random.randn(n)

    return X, y


# =============================================================================
# SIMULATION STUDIES
# =============================================================================

def simulate_type1_error(
    n_cal: int = 200,
    n_test: int = 200,
    d: int = 5,
    n_simulations: int = 500,
    beta: float = 0.05,
    interval_method: PredictionIntervalMethod = None
) -> Tuple[float, np.ndarray]:
    """
    Simulate Type I error rate under H0 (no shift).

    Returns: (type1_error_rate, test_statistics)
    """
    if interval_method is None:
        interval_method = QuantileRegressionInterval()

    rejections = 0
    test_stats = []

    for _ in tqdm(range(n_simulations), desc="Type I Error Simulation"):
        # Generate data under H0 (same distribution)
        X_all, y_all = generate_regression_data(n_cal + n_test, d)

        X_cal, y_cal = X_all[:n_cal], y_all[:n_cal]
        X_test = X_all[n_cal:]

        result = detect_shift(
            X_cal, y_cal, X_test,
            interval_method=interval_method,
            beta=beta,
            n_permutations=500
        )

        if result.shift_detected:
            rejections += 1
        test_stats.append(result.test_statistic)

    return rejections / n_simulations, np.array(test_stats)


def simulate_power_curve(
    shift_magnitudes: np.ndarray,
    n_cal: int = 200,
    n_test: int = 200,
    d: int = 5,
    n_simulations: int = 200,
    beta: float = 0.05,
    interval_method: PredictionIntervalMethod = None
) -> np.ndarray:
    """
    Simulate power curve across different shift magnitudes.

    Returns: power at each shift magnitude
    """
    if interval_method is None:
        interval_method = QuantileRegressionInterval()

    powers = []

    for shift in tqdm(shift_magnitudes, desc="Power Curve Simulation"):
        rejections = 0

        for _ in range(n_simulations):
            # Calibration data (no shift)
            X_cal, y_cal = generate_regression_data(n_cal, d, mean_shift=0.0)

            # Test data (with shift)
            X_test, _ = generate_regression_data(n_test, d, mean_shift=shift)

            result = detect_shift(
                X_cal, y_cal, X_test,
                interval_method=interval_method,
                beta=beta,
                n_permutations=500
            )

            if result.shift_detected:
                rejections += 1

        powers.append(rejections / n_simulations)

    return np.array(powers)


def validate_asymptotic_distribution(
    n_cal: int = 500,
    n_test: int = 500,
    d: int = 5,
    n_simulations: int = 1000,
    interval_method: PredictionIntervalMethod = None
) -> Tuple[np.ndarray, float]:
    """
    Validate that the asymptotic distribution of T_WR matches theory.

    Under H0: sqrt(nm/(n+m)) * (T_WR - 1) ~ N(0, sigma_W^2/mu_W^2)

    Returns: (normalized_statistics, ks_p_value)
    """
    if interval_method is None:
        interval_method = QuantileRegressionInterval()

    normalized_stats = []

    for _ in tqdm(range(n_simulations), desc="Asymptotic Validation"):
        # Generate data under H0
        X_all, y_all = generate_regression_data(n_cal + n_test, d)
        X_cal, y_cal = X_all[:n_cal], y_all[:n_cal]
        X_test = X_all[n_cal:]

        # Fit and get widths
        interval_method.fit(X_cal, y_cal)
        widths_cal = interval_method.get_widths(X_cal)
        widths_test = interval_method.get_widths(X_test)

        # Compute normalized statistic
        t_wr = compute_width_ratio_statistic(widths_cal, widths_test)

        # Estimate variance ratio
        all_widths = np.concatenate([widths_cal, widths_test])
        mu_w = np.mean(all_widths)
        sigma_w = np.std(all_widths)
        cv_w = sigma_w / mu_w

        # Normalize
        scaling = np.sqrt(n_cal * n_test / (n_cal + n_test))
        z = scaling * (t_wr - 1) / cv_w

        normalized_stats.append(z)

    normalized_stats = np.array(normalized_stats)

    # KS test against standard normal
    ks_stat, ks_pvalue = kstest(normalized_stats, 'norm')

    return normalized_stats, ks_pvalue


def compare_interval_methods(
    shift_magnitudes: np.ndarray,
    n_cal: int = 200,
    n_test: int = 200,
    d: int = 5,
    n_simulations: int = 100,
    beta: float = 0.05
) -> Dict[str, np.ndarray]:
    """
    Compare different prediction interval methods for shift detection.
    """
    methods = {
        'Quantile Regression': QuantileRegressionInterval(),
        'Bootstrap': BootstrapInterval(n_bootstrap=50),
        'Heteroscedastic': HeteroscedasticInterval(),
        'Conformal': ConformalInterval()
    }

    results = {}

    for name, method in methods.items():
        print(f"\nEvaluating {name}...")
        powers = simulate_power_curve(
            shift_magnitudes,
            n_cal=n_cal,
            n_test=n_test,
            d=d,
            n_simulations=n_simulations,
            beta=beta,
            interval_method=method
        )
        results[name] = powers

    return results


# =============================================================================
# VISUALIZATION
# =============================================================================

def plot_type1_error_validation(
    test_stats: np.ndarray,
    beta: float = 0.05,
    save_path: Optional[str] = None
):
    """Plot Type I error validation results."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Histogram of test statistics
    axes[0].hist(test_stats, bins=30, density=True, alpha=0.7, edgecolor='black')
    axes[0].axvline(x=1.0, color='r', linestyle='--', label='Expected under H0')
    axes[0].set_xlabel('Width Ratio Statistic $T_{WR}$')
    axes[0].set_ylabel('Density')
    axes[0].set_title('Distribution of Test Statistic Under H0')
    axes[0].legend()

    # QQ plot
    stats.probplot(test_stats, dist="norm", plot=axes[1])
    axes[1].set_title('Q-Q Plot of Test Statistics')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()


def plot_power_curve(
    shift_magnitudes: np.ndarray,
    powers: np.ndarray,
    beta: float = 0.05,
    theoretical_power: Optional[np.ndarray] = None,
    save_path: Optional[str] = None
):
    """Plot power curve."""
    plt.figure(figsize=(8, 5))

    plt.plot(shift_magnitudes, powers, 'o-', label='Empirical Power', linewidth=2)

    if theoretical_power is not None:
        plt.plot(shift_magnitudes, theoretical_power, '--',
                label='Theoretical Power', linewidth=2)

    plt.axhline(y=beta, color='r', linestyle=':', label=f'Type I Error = {beta}')
    plt.axhline(y=0.8, color='g', linestyle=':', alpha=0.5, label='80% Power')

    plt.xlabel('Shift Magnitude (||μ||)')
    plt.ylabel('Power')
    plt.title('Power Curve for Width-Based Shift Detection')
    plt.legend()
    plt.grid(True, alpha=0.3)

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()


def plot_asymptotic_validation(
    normalized_stats: np.ndarray,
    ks_pvalue: float,
    save_path: Optional[str] = None
):
    """Plot asymptotic distribution validation."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Histogram vs standard normal
    x = np.linspace(-4, 4, 100)
    axes[0].hist(normalized_stats, bins=40, density=True, alpha=0.7,
                edgecolor='black', label='Empirical')
    axes[0].plot(x, norm.pdf(x), 'r-', linewidth=2, label='N(0,1)')
    axes[0].set_xlabel('Normalized Statistic')
    axes[0].set_ylabel('Density')
    axes[0].set_title(f'Asymptotic Distribution (KS p-value: {ks_pvalue:.3f})')
    axes[0].legend()

    # QQ plot
    stats.probplot(normalized_stats, dist="norm", plot=axes[1])
    axes[1].set_title('Q-Q Plot vs Standard Normal')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()


def plot_method_comparison(
    shift_magnitudes: np.ndarray,
    results: Dict[str, np.ndarray],
    beta: float = 0.05,
    save_path: Optional[str] = None
):
    """Plot comparison of different interval methods."""
    plt.figure(figsize=(10, 6))

    markers = ['o', 's', '^', 'D']
    for (name, powers), marker in zip(results.items(), markers):
        plt.plot(shift_magnitudes, powers, marker=marker, label=name, linewidth=2)

    plt.axhline(y=beta, color='r', linestyle=':', label=f'Type I Error = {beta}')
    plt.axhline(y=0.8, color='g', linestyle=':', alpha=0.5)

    plt.xlabel('Shift Magnitude (||μ||)')
    plt.ylabel('Power')
    plt.title('Power Comparison: Different Prediction Interval Methods')
    plt.legend()
    plt.grid(True, alpha=0.3)

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()


# =============================================================================
# THEORETICAL POWER COMPUTATION
# =============================================================================

def compute_theoretical_power(
    shift_magnitudes: np.ndarray,
    n_cal: int,
    n_test: int,
    cv_w: float,
    beta: float = 0.05
) -> np.ndarray:
    """
    Compute theoretical power under local alternatives.

    Power = Phi(delta / (CV_W) * sqrt(n/(n+m)) - z_{1-beta/2})
          + Phi(-delta / (CV_W) * sqrt(n/(n+m)) - z_{1-beta/2})

    For simplicity, we approximate delta ≈ shift_magnitude * sensitivity
    """
    z_crit = norm.ppf(1 - beta/2)

    # Approximate sensitivity (how much width changes per unit shift)
    # This is problem-dependent; we estimate empirically
    sensitivity = 0.3  # Approximate from simulations

    delta_over_cv = shift_magnitudes * sensitivity / cv_w
    scaling = np.sqrt(n_cal / (n_cal + n_test))

    noncentrality = delta_over_cv * scaling

    power = (1 - norm.cdf(z_crit - noncentrality) +
             norm.cdf(-z_crit - noncentrality))

    return power


# =============================================================================
# MAIN SIMULATION RUNNER
# =============================================================================

def run_all_simulations(
    output_dir: str = './simulation_results'
):
    """Run all simulation studies and generate figures."""
    import os
    os.makedirs(output_dir, exist_ok=True)

    print("=" * 60)
    print("WIDTH-BASED SHIFT DETECTION: SIMULATION STUDIES")
    print("=" * 60)

    # Parameters
    n_cal = 200
    n_test = 200
    d = 5
    beta = 0.05

    # -----------------------------------------------------------------
    # Study 1: Type I Error Control
    # -----------------------------------------------------------------
    print("\n" + "=" * 60)
    print("STUDY 1: TYPE I ERROR CONTROL")
    print("=" * 60)

    type1_error, test_stats = simulate_type1_error(
        n_cal=n_cal, n_test=n_test, d=d,
        n_simulations=500, beta=beta
    )

    print(f"\nEmpirical Type I Error: {type1_error:.4f}")
    print(f"Nominal Level (beta): {beta}")
    print(f"Type I Error Control: {'PASSED' if type1_error <= beta + 0.02 else 'FAILED'}")

    plot_type1_error_validation(
        test_stats, beta,
        save_path=f"{output_dir}/type1_error_validation.png"
    )

    # -----------------------------------------------------------------
    # Study 2: Power Curve
    # -----------------------------------------------------------------
    print("\n" + "=" * 60)
    print("STUDY 2: POWER CURVE")
    print("=" * 60)

    shift_magnitudes = np.array([0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0])

    powers = simulate_power_curve(
        shift_magnitudes,
        n_cal=n_cal, n_test=n_test, d=d,
        n_simulations=200, beta=beta
    )

    print("\nShift Magnitude vs Power:")
    for shift, power in zip(shift_magnitudes, powers):
        print(f"  ||μ|| = {shift:.1f}: Power = {power:.3f}")

    # Theoretical power (approximate)
    cv_w_est = 0.5  # Estimated from simulations
    theoretical_power = compute_theoretical_power(
        shift_magnitudes, n_cal, n_test, cv_w_est, beta
    )

    plot_power_curve(
        shift_magnitudes, powers, beta,
        theoretical_power=theoretical_power,
        save_path=f"{output_dir}/power_curve.png"
    )

    # -----------------------------------------------------------------
    # Study 3: Asymptotic Distribution Validation
    # -----------------------------------------------------------------
    print("\n" + "=" * 60)
    print("STUDY 3: ASYMPTOTIC DISTRIBUTION VALIDATION")
    print("=" * 60)

    normalized_stats, ks_pvalue = validate_asymptotic_distribution(
        n_cal=500, n_test=500, d=d,
        n_simulations=1000
    )

    print(f"\nKolmogorov-Smirnov Test vs N(0,1):")
    print(f"  KS p-value: {ks_pvalue:.4f}")
    print(f"  Asymptotic Normality: {'CONFIRMED' if ks_pvalue > 0.05 else 'REJECTED'}")

    plot_asymptotic_validation(
        normalized_stats, ks_pvalue,
        save_path=f"{output_dir}/asymptotic_validation.png"
    )

    # -----------------------------------------------------------------
    # Study 4: Method Comparison
    # -----------------------------------------------------------------
    print("\n" + "=" * 60)
    print("STUDY 4: PREDICTION INTERVAL METHOD COMPARISON")
    print("=" * 60)

    comparison_results = compare_interval_methods(
        shift_magnitudes,
        n_cal=n_cal, n_test=n_test, d=d,
        n_simulations=100, beta=beta
    )

    print("\nPower at shift = 2.0:")
    for name, powers in comparison_results.items():
        idx = np.where(shift_magnitudes == 2.0)[0][0]
        print(f"  {name}: {powers[idx]:.3f}")

    plot_method_comparison(
        shift_magnitudes, comparison_results, beta,
        save_path=f"{output_dir}/method_comparison.png"
    )

    # -----------------------------------------------------------------
    # Summary
    # -----------------------------------------------------------------
    print("\n" + "=" * 60)
    print("SIMULATION SUMMARY")
    print("=" * 60)
    print(f"""
Results saved to: {output_dir}/

Key Findings:
1. Type I Error: {type1_error:.4f} (nominal: {beta})
2. Power at shift=2.0: {powers[4]:.3f}
3. Asymptotic normality: {'Confirmed' if ks_pvalue > 0.05 else 'Rejected'} (KS p={ks_pvalue:.3f})
4. Best method: {max(comparison_results.items(), key=lambda x: np.mean(x[1]))[0]}
""")


# =============================================================================
# QUICK DEMO
# =============================================================================

def quick_demo():
    """Quick demonstration of the shift detection method."""
    print("=" * 60)
    print("QUICK DEMO: Width-Based Shift Detection")
    print("=" * 60)

    # Generate data
    np.random.seed(123)
    n_cal, n_test, d = 200, 200, 5

    # Calibration data (no shift)
    X_cal, y_cal = generate_regression_data(n_cal, d, mean_shift=0.0)

    # Test data (with shift)
    X_test_no_shift, _ = generate_regression_data(n_test, d, mean_shift=0.0)
    X_test_with_shift, _ = generate_regression_data(n_test, d, mean_shift=2.0)

    # Test without shift
    print("\n--- Test WITHOUT Shift ---")
    result_no_shift = detect_shift(
        X_cal, y_cal, X_test_no_shift,
        interval_method=QuantileRegressionInterval(),
        beta=0.05,
        n_permutations=1000
    )
    print(f"Test statistic: {result_no_shift.test_statistic:.4f}")
    print(f"P-value: {result_no_shift.p_value:.4f}")
    print(f"Shift detected: {result_no_shift.shift_detected}")
    print(f"Mean width (cal): {result_no_shift.mean_width_cal:.4f}")
    print(f"Mean width (test): {result_no_shift.mean_width_test:.4f}")

    # Test with shift
    print("\n--- Test WITH Shift (||μ|| = 2.0) ---")
    result_with_shift = detect_shift(
        X_cal, y_cal, X_test_with_shift,
        interval_method=QuantileRegressionInterval(),
        beta=0.05,
        n_permutations=1000
    )
    print(f"Test statistic: {result_with_shift.test_statistic:.4f}")
    print(f"P-value: {result_with_shift.p_value:.4f}")
    print(f"Shift detected: {result_with_shift.shift_detected}")
    print(f"Mean width (cal): {result_with_shift.mean_width_cal:.4f}")
    print(f"Mean width (test): {result_with_shift.mean_width_test:.4f}")


# =============================================================================
# ENTRY POINT
# =============================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Width-Based Shift Detection Simulations"
    )
    parser.add_argument(
        "--mode",
        choices=["demo", "full"],
        default="demo",
        help="Run quick demo or full simulations"
    )
    parser.add_argument(
        "--output-dir",
        default="./simulation_results",
        help="Directory for saving results"
    )

    args = parser.parse_args()

    if args.mode == "demo":
        quick_demo()
    else:
        run_all_simulations(output_dir=args.output_dir)
